{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f531253e",
   "metadata": {},
   "source": [
    "## Theory: Introduction to Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd95be6",
   "metadata": {},
   "source": [
    "In this blog we will learn about topic modelling (or how to identify topics in a given corpus of text).\n",
    "\n",
    "The problem of topic modeling has become increasingly important in the last decade as our society has become more digitalized, with increased use of online platforms such as social media, which, combined with advances in storage capacity technologies, has resulted in an exponential increase in the amount of unstructured data available to us, with text data being one of the most common types.\n",
    "\n",
    "However, all this data that is becoming increasing available has a lot of information associated with it, which makes it difficult for us to find exactly what we're looking for. As an analogy, finding a certain sentence on a page is considerably easier than finding it in an entire book.\n",
    "\n",
    "So, we need tools and techniques to organise/search/understand the vast quantities of information.\n",
    "1. Firstly, we need to organise the data. Because at this moment most of the data is not organised in any manner.\n",
    "    - ex: text data on social media platforms (twitter/instagram) is unorganised\n",
    "    - ex: text data on customer reviews is unorganised\n",
    "2. Secondly, wee need to be able to search from it\n",
    "3. Thirdly, understand the information that is present in the data\n",
    "\n",
    "Topic modelling provide us with methods to (organise/search/understand) by summarising the logical actions of textual information that is present in the data. It helps:\n",
    "1. Discover the hidden topical patterns that are present in the collection\n",
    "2. Annotate the data, so that it can be used further on based on the topics that have been identified\n",
    "\n",
    "Topic modelling can also be described as a method for finding a group of words from a collection of documents that best represents the information in the collection. It can also be thought of as a form of text mining, because you're mining a vast amount of textual information. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7006f6e7",
   "metadata": {},
   "source": [
    "In order to do this, we will be using unsupervised machine learning techniques because our data is unlabelled, which is help us in clustering/grouping this data (ex: customer reviews) to identify the main ideas/topics in a corpus of text.\n",
    "\n",
    "In this blog we will use a real-world twitter textual corpus of data. However, note that the same techniques will apply to any corpus of text. This corpus of text was web-scraped directly from the twitter data and therefore it will have all the characteristics that any real-world corpus of text will have with things like: people have their own coloquial language, presence of noise. So, we will learn how to clean the noise and use that data for clustering to identify the main topics that people are talking about. Then, depending on whatever is the business decision, they can take certain steps towards it.\n",
    "\n",
    "In order to do so, I will be using the NLTK (Natural Language Toolkit) package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b6f866",
   "metadata": {},
   "source": [
    "## Theory: Introduction to NLTK (Natural Language Toolkit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08e03d5",
   "metadata": {},
   "source": [
    "NLTK provides us with tools that enable us to make the computer understand natural language. It's the leading library for building python programmes to work with human language data.\n",
    "\n",
    "For a computer, it's very easy to interpret programming language as they follow a specific syntax enabling the computer to parse it with any problems. However, human language is a very unstructured form of data, which means that the same thing can be said in a variety of ways and it means the same thing. \n",
    "\n",
    "- For example, sentences \"I hope you're doing well\" and \"I hope everything is fine with you\", both have the same meaning and use similar words, however, the way they're structured is different. Hence, for a computer these two sentence are basically different sentences which can mean different things. \n",
    "\n",
    "- NLTK is one of the tools that help computers clean and preprocess the human language data in such a way that makes it more structured such that computers can understand it.\n",
    "\n",
    "NLTK provides quite an easy to use interface and it has a suite of text processing libraries, for things like:\n",
    "- **Classification**\n",
    "- **Tokenisation** => sepparating out the words and removing punctuation\n",
    "- **Stemming** => since same word can have prefixes/suffixes, stemming will cut all these prefixes/suffixes & get the core of the word which still has the same meaning (ex: words \"stemming\", \"stemmer\" => can be stemmed to the word \"stem\")\n",
    "- **Tagging** => each word can be gramatically tagged - whether it's (an article, a verb, a noun, an adjective, etc)\n",
    "\n",
    "Finally, the best thing is that NLTK is free and open source, so if you want you can even contribute to it as it is a community driven project. \n",
    "\n",
    "Here is the link to [GitHub Repository of NLTK](https://github.com/nltk/nltk)\n",
    "\n",
    "The way to use NLTK is just like any other python library, you can directly import it:\n",
    "```python\n",
    "import nltk\n",
    "```\n",
    "However, initially you would have to install NLTK and given that it has a vast suite of tools that are available for a variety of tasks - usually you don't require all of them and to install all of them would take you a very long time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb3d38b",
   "metadata": {},
   "source": [
    "## Practise: Loading and Exploring Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4d3812",
   "metadata": {},
   "source": [
    "## Practise: Cleaning the Data with Pattern Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfdd9fe",
   "metadata": {},
   "source": [
    "## Practise: Tokenise and Identify Special Instances of Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808eba82",
   "metadata": {},
   "source": [
    "## Practise: Vectoriser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d730abe2",
   "metadata": {},
   "source": [
    "## Theory: Understanding K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecebfbbf",
   "metadata": {},
   "source": [
    "## Practise: Clustering with 8 centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c27af0",
   "metadata": {},
   "source": [
    "## Practise: Clustering with 2 centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93417ecc",
   "metadata": {},
   "source": [
    "## Practise: Clustering with 2 centroids - Word Clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4266ff3",
   "metadata": {},
   "source": [
    "## Practise: General Function Homogeneity in Cluster - Finding the optimal Cluster Number"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
